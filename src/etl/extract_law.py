import time
import json
import os
import re
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

# --- AYARLAR ---
BASE_URL = "https://www.mevzuat.gov.tr/"
OUTPUT_DIR = os.path.join("data", "raw")
os.makedirs(OUTPUT_DIR, exist_ok=True)

def setup_driver():
    """Chrome sÃ¼rÃ¼cÃ¼sÃ¼nÃ¼ baÅŸlatÄ±r."""
    options = webdriver.ChromeOptions()
    # options.add_argument("--headless")
    options.add_argument("--start-maximized")
    options.add_argument("--disable-notifications")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
    return driver

def clean_text(text):
    """Metindeki fazla boÅŸluklarÄ± temizler."""
    return re.sub(r'\s+', ' ', text).strip()

def parse_law_detail(driver, law_url, law_title):
    """Kanun detay sayfasÄ±na gider ve tÃ¼m metni alÄ±r, sonra ayrÄ±ÅŸtÄ±rÄ±r."""
    try:
        print(f"   ğŸ“„ Sayfa yÃ¼kleniyor...")
        driver.get(law_url)

        # SayfanÄ±n yÃ¼klenmesini bekle - body elementi her zaman var
        wait = WebDriverWait(driver, 15)
        wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))

        # JavaScript render iÃ§in bekle
        time.sleep(3)

        # IFRAME'Ä° BUL VE GEÃ‡Ä°Å YAP
        try:
            print(f"   ğŸ”„ Iframe bekleniyor...")
            iframe = wait.until(EC.presence_of_element_located((By.ID, "mevzuatDetayIframe")))
            print(f"   âœ“ Iframe bulundu, geÃ§iÅŸ yapÄ±lÄ±yor...")
            driver.switch_to.frame(iframe)
            time.sleep(2)  # Iframe iÃ§eriÄŸinin yÃ¼klenmesi iÃ§in bekle
            print(f"   âœ“ Iframe iÃ§ine geÃ§ildi")
        except Exception as e:
            print(f"   âš ï¸ Iframe bulunamadÄ±: {e}")

        soup = BeautifulSoup(driver.page_source, "html.parser")

        # FarklÄ± iÃ§erik div'lerini dene (Ã¶ncelik sÄ±rasÄ±na gÃ¶re)
        content_div = None
        div_classes_to_try = [
            "WordSection1",
            "MevzuatMetin",
            "mevzuat-content",
            "kanun-metni"
        ]

        for cls in div_classes_to_try:
            content_div = soup.find("div", class_=cls)
            if content_div:
                print(f"   âœ“ Ä°Ã§erik bulundu: {cls}")
                break

        # EÄŸer hala bulunamadÄ±ysa, id="contentPane" veya iÃ§inde Ã§ok paragraf olan div'i bul
        if not content_div:
            print(f"âš ï¸  Class ile bulunamadÄ±, alternatif yÃ¶ntemler deneniyor...")

            # ID ile dene
            content_div = soup.find("div", id="contentPane")
            if content_div:
                print(f"   âœ“ contentPane bulundu")

            # Hala yoksa, en Ã§ok <p> iÃ§eren div'i al
            if not content_div:
                all_divs = soup.find_all("div")
                max_p_count = 0
                for div in all_divs:
                    p_count = len(div.find_all("p"))
                    if p_count > max_p_count:
                        max_p_count = p_count
                        content_div = div

                if content_div and max_p_count > 0:
                    print(f"   âœ“ En Ã§ok paragraf iÃ§eren div bulundu ({max_p_count} paragraf)")

        if not content_div:
            print(f"âš ï¸  UyarÄ±: HiÃ§bir iÃ§erik div'i bulunamadÄ±!")
            # HTML'i dosyaya kaydet (debug iÃ§in)
            debug_file = os.path.join(OUTPUT_DIR, "debug_page.html")
            with open(debug_file, "w", encoding="utf-8") as f:
                f.write(driver.page_source)
            print(f"   Debug iÃ§in HTML kaydedildi: {debug_file}")
            return "", "", []

        print(f"   âœ“ Ä°Ã§erik bulundu: {content_div.name}")

        # TÃ¼m p etiketlerini al
        all_paragraphs = content_div.find_all("p")
        print(f"   âœ“ {len(all_paragraphs)} paragraf bulundu")

        # TÃ¼m metni satÄ±r satÄ±r topla
        lines = []
        for p in all_paragraphs:
            text = clean_text(p.get_text())
            if text:
                lines.append(text)

        if not lines:
            print(f"âš ï¸  UyarÄ±: HiÃ§ metin bulunamadÄ±!")
            return "", "", []

        print(f"   âœ“ {len(lines)} satÄ±r metin Ã§ekildi")
        print(f"   Ä°lk satÄ±r: {lines[0][:80]}...")

        # 1. BAÅLIK: "Kanun NumarasÄ±:" kÄ±smÄ±na kadar olan bÃ¼yÃ¼k harfli kÄ±sÄ±m
        baslik_lines = []
        kanun_bilgileri_baslangic = -1

        for i, line in enumerate(lines):
            # "Kanun NumarasÄ±" veya "KANUN NUMARASI" bulundu mu?
            if re.search(r'Kanun\s+Numaras[Ä±i]', line, re.IGNORECASE):
                kanun_bilgileri_baslangic = i
                break
            # SatÄ±r Ã§oÄŸunlukla bÃ¼yÃ¼k harfse baÅŸlÄ±ÄŸa ekle
            upper_count = sum(1 for c in line if c.isupper())
            alpha_count = sum(1 for c in line if c.isalpha())
            if alpha_count > 0 and upper_count / alpha_count > 0.7:  # %70'i bÃ¼yÃ¼k harf
                baslik_lines.append(line)

        baslik = " ".join(baslik_lines).strip()
        print(f"   âœ“ BaÅŸlÄ±k: {baslik[:60]}...")

        # 2. KANUN BÄ°LGÄ°LERÄ°: "Kanun NumarasÄ±"ndan "MADDE"ye kadar
        kanun_bilgileri_lines = []
        maddeler_baslangic = -1

        if kanun_bilgileri_baslangic != -1:
            for i in range(kanun_bilgileri_baslangic, len(lines)):
                line = lines[i]
                # MADDE ile baÅŸlayan satÄ±r mÄ±?
                if re.match(r'^(MADDE|GEÃ‡Ä°CÄ°\s+MADDE)\s+\d+', line, re.IGNORECASE):
                    maddeler_baslangic = i
                    break
                kanun_bilgileri_lines.append(line)

        kanun_bilgileri = " ".join(kanun_bilgileri_lines).strip()
        print(f"   âœ“ Kanun Bilgileri: {kanun_bilgileri[:80]}...")

        # 3. MADDELER: "MADDE X" ile baÅŸlayan tÃ¼m satÄ±rlar
        maddeler = []

        if maddeler_baslangic != -1:
            current_madde = None

            for i in range(maddeler_baslangic, len(lines)):
                line = lines[i]

                # Yeni madde baÅŸlangÄ±cÄ± mÄ±?
                match = re.match(r'^(MADDE\s+\d+|GEÃ‡Ä°CÄ°\s+MADDE\s+\d+)', line, re.IGNORECASE)

                if match:
                    # Ã–nceki maddeyi kaydet
                    if current_madde and current_madde["icerik"]:
                        maddeler.append(current_madde)

                    # Yeni madde baÅŸlat
                    madde_no = match.group(1).strip()
                    current_madde = {
                        "madde_no": madde_no,
                        "icerik": line
                    }
                else:
                    # Devam eden satÄ±rÄ± ekle
                    if current_madde:
                        current_madde["icerik"] += " " + line

            # Son maddeyi ekle
            if current_madde and current_madde["icerik"]:
                maddeler.append(current_madde)

        print(f"   âœ“ {len(maddeler)} madde bulundu")

        return baslik, kanun_bilgileri, maddeler

    except Exception as e:
        print(f"âŒ Detay sayfasÄ± hatasÄ± ({law_url}): {e}")
        import traceback
        traceback.print_exc()
        return "", "", []

def main():
    driver = setup_driver()
    all_laws_data = []

    try:
        print(f"ğŸŒ Siteye gidiliyor: {BASE_URL}")
        driver.get(BASE_URL + "#kanunlar")

        wait = WebDriverWait(driver, 15)

        print("ğŸ–±ï¸  'TÃ¼m Kanunlar' butonuna tÄ±klanÄ±yor...")
        try:
            show_all_btn = wait.until(EC.element_to_be_clickable(
                (By.CSS_SELECTOR, ".btn.btn-secondary.text-light.float-right.ml-1")
            ))
            show_all_btn.click()
        except:
            print("âš ï¸  Buton CSS ile bulunamadÄ±, alternatif XPath deneniyor...")
            show_all_btn = driver.find_element(By.XPATH, "//a[contains(text(),'TÃ¼mÃ¼ne Git')]")
            show_all_btn.click()

        print("â³ Tablo yÃ¼kleniyor...")
        time.sleep(3)

        law_links = driver.find_elements(By.CSS_SELECTOR, "a.ml-1")
        print(f"ğŸ“‹ Bu sayfada {len(law_links)} kanun bulundu. Ä°lk 3 tanesi Ã§ekilecek.")

        target_urls = []
        for link in law_links[:3]:
            url = link.get_attribute("href")
            title = link.text.split('\n')[0].strip()
            if url:
                target_urls.append({"title": title, "url": url})

        # Her bir kanunun detayÄ±na git
        for idx, item in enumerate(target_urls, 1):
            print(f"\n{'='*60}")
            print(f"[{idx}/{len(target_urls)}] Ä°ÅŸleniyor: {item['title']}")
            print(f"{'='*60}")

            baslik, kanun_bilgileri, maddeler = parse_law_detail(driver, item['url'], item['title'])

            law_record = {
                "kanun_adi": baslik if baslik else item['title'],
                "kanun_bilgileri": kanun_bilgileri,
                "url": item['url'],
                "maddeler": maddeler
            }
            all_laws_data.append(law_record)

            print(f"âœ“ TamamlandÄ±!")
            time.sleep(2)

        # Kaydet
        output_file = os.path.join(OUTPUT_DIR, "kanunlar_selenium.json")
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(all_laws_data, f, ensure_ascii=False, indent=2)

        print(f"\n{'='*60}")
        print(f"âœ… Ä°ÅŸlem Tamam! Veriler kaydedildi: {output_file}")
        print(f"ğŸ“Š Toplam {len(all_laws_data)} kanun iÅŸlendi.")

        # Ã–zet gÃ¶ster
        for law in all_laws_data:
            print(f"\nğŸ“Œ {law['kanun_adi'][:60]}...")
            print(f"   - Madde sayÄ±sÄ±: {len(law['maddeler'])}")

    except Exception as e:
        print(f"âŒ Genel Hata: {e}")
        import traceback
        traceback.print_exc()
    finally:
        driver.quit()

if __name__ == "__main__":
    main()